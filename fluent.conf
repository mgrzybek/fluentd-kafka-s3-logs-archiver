<source>
  @type kafka_group

  brokers "#{ENV['broker_servers']']"
  consumer_group "#{ENV['consumer_group']"
  topics "#{ENV['topics']" # comma-separated
  format json
  time_source record

  # ruby-kafka consumer options
  max_bytes                "#{ENV['max_bytes']"
  #max_wait_time           "#{ENV['ax_wait_time']"
  #min_bytes               "#{ENV['min_bytes']"
  #offset_commit_interval  "#{ENV['offset_commit_interval']"
  #offset_commit_threshold "#{ENV['offset_commit_threshold']"
  #fetcher_max_queue_size  "#{ENV['fetcher_max_queue_size']"
  #refresh_topic_interval  "#{ENV['refresh_topic_interval']"
  start_from_beginning     "#{ENV['start_from_beginning']"
</source>

<match **>
  @type s3
  aws_key_id "#{ENV['access_key']"
  aws_sec_key "#{ENV['secret_key']"
  s3_bucket "#{ENV['bucket_name']"
  s3_endpoint "#{ENV['endpoint']"
  s3_region "#{ENV['region']"
  #path logs/                      # This prefix is added to each file
  force_path_style true            # This prevents AWS SDK from breaking endpoint URL
  time_slice_format %Y-%m-%d-%H-%M # This timestamp is added to each file name

  <buffer time>
    @type file
    path "#{ENV['path']"
    timekey "#{ENV['timekey']"                   # Flush the accumulated chunks every hour
    timekey_wait "#{ENV['timekey_wait']"         # Wait for 60 seconds before flushing
    timekey_use_utc "#{ENV['timekey_use_utc']"   # Use this option if you prefer UTC timestamps
    chunk_limit_size "#{ENV['chunk_limit_size']" # The maximum size of each chunk
  </buffer>
</match>
